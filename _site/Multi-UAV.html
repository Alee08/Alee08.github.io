<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- <title>Multi-UAV reinforcement learning with temporal and priority goals</title> -->
<!-- name and description are set by Jekyll SEO -->
<meta name = "robots" content = "index, follow">
<meta name = "keywords" content = "alessandro Trapasso, computer science, artificial intelligence, research, sapienza">

<meta property="og:type" content="website"> 

<link rel="stylesheet" href="/assets/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="https://code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">

<link rel="canonical" href="https://alee08.github.io/Multi-UAV">
<link rel="alternate" type="application/rss+xml" title="Alessandro Trapasso" href="/feed.xml">



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multi-UAV reinforcement learning with temporal and priority goals | Alessandro Trapasso</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Multi-UAV reinforcement learning with temporal and priority goals" />
<meta name="author" content="Alessandro Trapasso" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Master of Science in Engineering in Computer Science" />
<meta property="og:description" content="Master of Science in Engineering in Computer Science" />
<link rel="canonical" href="https://alee08.github.io/Multi-UAV" />
<meta property="og:url" content="https://alee08.github.io/Multi-UAV" />
<meta property="og:site_name" content="Alessandro Trapasso" />
<meta property="og:image" content="https://alee08.github.io/_pages/Multi_UAV/scenario.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-17T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://alee08.github.io/_pages/Multi_UAV/scenario.png" />
<meta property="twitter:title" content="Multi-UAV reinforcement learning with temporal and priority goals" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Multi-UAV reinforcement learning with temporal and priority goals","dateModified":"2021-04-17T00:00:00+02:00","datePublished":"2021-04-17T00:00:00+02:00","url":"https://alee08.github.io/Multi-UAV","mainEntityOfPage":{"@type":"WebPage","@id":"https://alee08.github.io/Multi-UAV"},"author":{"@type":"Person","name":"Alessandro Trapasso"},"image":"https://alee08.github.io/_pages/Multi_UAV/scenario.png","description":"Master of Science in Engineering in Computer Science","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Multi-UAV reinforcement learning with temporal and priority goals",
      "description": "Master of Science in Engineering in Computer Science",

      "published": "2021-04-16",
      "authors": [
        
        {
          "author": "Alessandro Trapasso",
          "authorURL": "https://alee08.github.io",
          "affiliations": [
            
          ]
        },
        
        {
          "author": "Supervised by Prof. Luca Iocchi",
          "authorURL": "https://sites.google.com/a/dis.uniroma1.it/iocchi/home",
          "affiliations": [
            
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
      
      

    }</script>
  </d-front-matter>

  <body class="page">

    <!-- Header -->

    <header class="site-header">

  <div class="wrapper"><!--
    <a class="site-title" rel="author" href="/">Alessandro Trapasso</a>
	--><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            
            <a class="page-link " href="/">About</a>
            
            <a class="page-link " href="/Multi-UAV">Multi-UAV RL with temporal &amp; priority goals</a>
            
        </div>
      </nav></div>
</header>



    <!-- Content -->
    <main class="page-content" aria-label="Content">
    <div class="post distill">

      <d-title>
        <h1>Multi-UAV reinforcement learning with temporal and priority goals</h1>
        <p>Master of Science in Engineering in Computer Science</p>
      </d-title>

      <d-byline></d-byline>

      <d-article class="l-body">
        <p>Unmanned aerial vehicles (UAVs) are becoming more and more in demand, their use initially spread for the execution of military missions, now it is becoming increasingly popular in the civilian field. UAVs are involved in the most exterminated missions. In this thesis, we address the problem of the risk of conflicts between UAVs and then analyze it in a specific scenario. 
Minimizing the risk of collision is extensively covered in the <a href="http://bubbles-project.eu/"><em>BUBBLES project</em></a>.
The <a href="http://bubbles-project.eu/"><em>BUBBLES project</em></a> is a large project funded by the <em>European community</em>, coordinated by the <em>Universitat Politècnica de València</em> with the participation of the <a href="https://www.dis.uniroma1.it/en"><em>DIAG</em></a>, <a href="https://www.uniroma1.it/en/pagina-strutturale/home"><em>Sapienza University of Rome</em></a> coordinated by <a href="https://sites.google.com/a/dis.uniroma1.it/iocchi/home">Prof. Luca Iocchi</a>. 
The focus of the <em>Sapienza unit</em> is to implement methods to generate many trajectories for risk assessment.</p>

<h2 id="scenario-characteristics">Scenario Characteristics</h2>
<p>As it was previously said, the application scenarios in which <em>UAVs</em> can be used are very many, the scenario that is examined in this thesis project concerns the delivery of medical supplies between hospitals in a metropolis.
In our scenario, hospitals in the city of Rome are represented on the grid. Each hospital can request medicines, organs, vaccines, blood from one of the other $\mathcal{n-1}$ hospitals in the city.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="_pages\Multi_UAV\scenario.png" alt="" height="80%" width="80%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Scenario</td>
    </tr>
  </tbody>
</table>

<p>Each <em>UAV</em> must take the resource from a specific hospital and deliver it to the hospital that requested it.
UAVs must visit hospitals in a given order and must respect the mission priority assigned to them.</p>

<p>Another main goal is to reduce to 0 or in any case minimize the possible collisions between UAVs.</p>

<h2 id="objectives-of-the-abstract-problem">Objectives of the abstract problem</h2>
<p>The objectives of the abstract problem are:</p>
<ol>
  <li>Find an optimal policy for each agent, reducing interference between agent policies</li>
  <li>Agents must respect the mission priority associated with each goal and reach the goals in the correct order</li>
</ol>

<h2 id="assumptions">Assumptions</h2>
<ul>
  <li>We assume a global clock, which measures the time during the evolution of the scenario for each agent $\alpha_i$, from 0 to T.</li>
  <li>Let’s consider a set of asynchronous goals, in which each agent can decide to execute its task at any time independently from the other agents.</li>
  <li>Each agent does not know where the other agents are at any given instant of time.</li>
</ul>

<h2 id="problem-definitions">Problem Definitions</h2>
<p>The scenario of our case study is a tuple defined as follows:</p>

<p>A Scenario $\mathcal{S}  = \langle \mathcal{A, M, RB} \rangle $.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="_pages\Multi_UAV\problem.gif" alt="" height="100%" width="100%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Scenario definition</td>
    </tr>
  </tbody>
</table>

<p>Where $\mathcal{A}$ is the set of agent $ \alpha_i $ modelled by the MDP, in our case each agent is a UAV.
$\mathcal{RB}$ is the set of Restraining Bolts, which define the behaviour of the UAV to reach the goal.
The agent interacts with the $ W $ world through actions $ A_i $.</p>

<h2 id="restraining-bolt-augmented">Restraining Bolt augmented</h2>
<p>$\mathcal{RB_i^\mathcal{+}}=\mathcal\langle\ \mathcal{L}, (\varphi_{i}, r_{i}, P_{i}) \rangle$</p>

<p>The behaviour of each agent is learned through the use of <em>Reinforcement Learning</em> and <em>Restraining Bolt</em>.
Restraining Bolt was introduced in 2019 by (De Giacomo et al., 2019)<d-cite key="restraining"> restraining </d-cite> , this thesis work extends the original $\mathcal{RB}$ by adding the concept of priority.</p>

<p>Each $\mathcal{RB}$ is a tuple that contains:</p>
<ul>
  <li>$\mathcal{L}$ is the set of fluids,  in the case study is the colours assigned to hospitals</li>
  <li>each agent $\alpha_i$ has a goal ($\phi_i, P_i$) where is the <em>LTLf/LDLf</em> formula that the agent must satisfy.</li>
  <li>$P_i$ is a non-negative integer associated with this formula to indicate its priority.</li>
  <li>$r_i$ is the reward associated with formula $\phi_i$</li>
</ul>

<h3 id="reinforcement-learning--restraining-bolt-augmented">Reinforcement Learning + Restraining Bolt augmented</h3>
<p>In the <em>RL</em> the agent performs an action and receives a reward and an observation from the environment. With the <em>Bolt</em>, the agent receives two independent rewards, one from the <em>MDP</em> for each <em>state/action</em> and another from the $\mathcal{RB}$ based on the state of the automata, which follows the satisfaction state of the $\phi_i$, formula.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="_pages\Multi_UAV\RL-RB+.gif" alt="" height="100%" width="100%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Reinforcement Learning + Restraining Bolt augmented</td>
    </tr>
  </tbody>
</table>

<p>Each agent will try to maximize the rewards to find an optimal policy.
The concept of priority permits finding incremental policies. The system is scalable, and each agent learns independently.</p>

<h2 id="proposed-solution">Proposed Solution</h2>
<p>Minimizing the risk of collision is a fundamental requirement to carry out <em>Multi-UAV</em> missions safely, this requirement is more important than efficiency and must be guaranteed.
Another fundamental objective of this thesis is the management of missions according to their priority.
Hence, each <em>UAV</em> must reach its goal in the shortest possible time based on the priority of the task assigned to it, ensuring <em>safety</em>.</p>

<p>Our solution is given to find  $n$ policies $\rho_{\mathcal{a g}_{i}}$, one for each agent $\alpha_i$, that are individually optimal concerning each agent’s goal, and that in the case of conflicts, they are always resolved in favour of the agent who has a goal with higher priority (lower value of $P_i$).</p>

<p>To eliminate interference between agent policies, we define a new <em>“no-interference” reward function</em> $ N_{\mathcal{ag}_{i}}$ for each agent.
The $ N_{\mathcal{ag}_{i}}$ function is computed by applying, to the original reward function  $R_{\mathcal{ag}_{i}}$  a modifier $\mathcal{\tau_i}$ that depends on the trajectories generated by the execution of the optimal policy $\rho_{\mathcal{a g}_{i}}$ of agents with higher priority goals.
Each agent must satisfy its $\varphi_{i}$ formula and must respect the priority value assigned to this goal.</p>

<ul>
  <li>If the agent $ \alpha_i $ has $ P_i = 0 $ (highest priority), then $ N_{\mathcal{ag}_{i}}$ equals the reward function $ R_{\mathcal{ag}_{i}}$.</li>
  <li>If the agent $ \alpha_i $ has $ P_i &gt; 0 $, then $ N_{\mathcal{ag}_{i}} $ reward function is the sum of the reward function $ R_{\mathcal{ag}_{i}}$ of the agent $\alpha_i$ and of a modifier $\mathcal{\tau_i}$ that depends on the trajectories generated by optimal policies of agents $\alpha_j$ for which $P_j &lt; P_i$.</li>
</ul>

<p><em>More formaly:</em></p>
<p style="text-align: center;">
$N_{\mathcal{ag}_{i}} = R_{\mathcal{a g}_{i}} + f ( { \tau_j | \alpha_j \ s.t.\ P_j &lt; P_i })$
</p>

<p>$ N_{\mathcal{ag}_{i}} $ is defined on the basis of the policies obtained from the agents $\alpha_j$, who have a higher priority $P_j &lt; P_i$.</p>

<p>Note that, we admit non-deterministic behaviour during optimal policy execution. However, we assume that the variability of the trajectories is limited so that it is possible to define a modifier function $f$ removing interference.</p>

<h2 id="case-study">Case study</h2>
<p>The case study is a model of our hospital’s problem in which the world is represented in a grid.
$\mathcal{RB_i}$ gives a positive reward if the agent does <em>hovering/beep</em> in the right order on the cells of the hospitals.
In our case, the no-interference reward has been implemented by adding negative weights on the cells crossed by the trajectories of the previous agents.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="_pages\Multi_UAV\LDLf.png" alt="" height="100%" width="100%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">The missions of each <em>UAV</em> is specified by <em>LDLf</em> formulas</td>
    </tr>
  </tbody>
</table>

<p>The <em>LTLf/LDLf</em> formulas specify the mission of each <em>UAV</em> and the temporal order of the goals.
The <em>RL</em> algorithm used for the experiments is <em>SARSA</em>, <em>State – action – reward – state – action</em>, but we should use also other RL algorithms.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="_pages\Multi_UAV\ex3.gif" alt="" /></th>
      <th style="text-align: center"><img src="_pages\Multi_UAV\ex4.gif" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Experiment 3. Without priority</td>
      <td style="text-align: center">Experiment 4. With priority</td>
    </tr>
  </tbody>
</table>

<p>As we can see in experiment 1 without priority, the trajectories of the UAVs intersect with each other. In experiment 2 we resolve these conflicts with priority and with our <em>no-interference reward functions</em>.
The purple <em>UAV</em> has the highest priority so its trajectory is the same in both experiments. The orange <em>UAV</em> has priority 1, and the grey <em>UAV</em> has priority 2.
It is possible to perform experiments by increasing the size of the map, the number of goals and agents.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Our system is scalable, and we can generate hundreds of trajectories at a lower cost than a standard Multi-agent algorithm, in which case the cost would be exponential.
Our method finds an optimal policy for each agent, reducing interference between agents policies.
From the execution of the policies, we generate trajectories with a lower frequency of conflict.</p>



      <hr>



      <!-- disqus comments -->
      

      </d-article>
      <a style="text-decoration:none" href="mailto:ale.trapasso@gmail.com" class="image-link"> <p style="text-align:center"> <img width="100" height="22"  style="margin-bottom:5px" src="gmail.png" border="0"> <span><b> ale.trapasso8@gmail.com</b></span></p></a>
      <a style="text-decoration:none" href="https://it.linkedin.com/in/alessandro-trapasso" class="image-link"> <p style="text-align:center"> <img width="100" height="25"  style="margin-bottom:8px" src="LI-Logo.png" border="0"> <span><b>Alessandro Trapasso</b></span></p></a>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="./_pages/Multi_UAV/Multi-UAV.bib">
      </d-bibliography>
    </div>
  </main>
    <!-- Footer -->

    <footer class="site-footer">

  <div class="wrapper">
  
    
      <span class="footer-icon">
         <a href="mailto:ale.trapasso8@gmail.com"><img width="24" height="24" src="/assets/img/icons/envelope.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="mailto:trapasso.1597997@studenti.uniroma1.it"><img width="24" height="24" src="/assets/img/icons/envelope.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://github.com/Alee08"><img width="24" height="24" src="/assets/img/icons/github.svg"></a>
      </span>
  
    
      <span class="footer-icon">
         <a href="https://www.linkedin.com/in/alessandro-trapasso/"><img width="24" height="24" src="/assets/img/icons/linkedin.svg"></a>
      </span>
   

<!-- <div style="display: inline; text-align: center"
  itemscope itemtype="https://schema.org/Person">
  <a itemprop="sameAs" content="https://orcid.org/"
     href="https://orcid.org/"
     target="orcid.widget"
     rel="me noopener noreferrer"
     style="vertical-align:top;">
      <img src="/assets/img/icons/orcid_bw.png"
           width="24" height="24"
           alt="ORCID iD icon"></a></div>
           https://orcid.org/
-->
</div>


</footer>


  </body>

  <!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap//js/mdb.min.js" integrity="" crossorigin="anonymous"></script> -->


  

  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script src="/assets/js/publications.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', '');
  
  gtag('set', 'anonymizeIp', true);
  gtag('send', 'pageview');
</script>




</html>